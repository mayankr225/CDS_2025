{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CDS Project: Part 1**\n",
        "\n",
        "*Institute of Software Security (E22)*  \n",
        "*Hamburg University of Technology*  \n",
        "*SoSe 2023*\n",
        "\n",
        "## Learning objectives\n",
        "---\n",
        "\n",
        "- Use a basic Machine Learning (ML) pipeline with pre-trained models.\n",
        "- Build your own data loader.\n",
        "- Load and run a pre-trained ML model.\n",
        "- Evaluate the performance of an ML model.\n",
        "- Calculate and interpret performance metrics.\n",
        "\n",
        "## Materials\n",
        "---\n",
        "\n",
        "- Lecture Slides 1, 2, and 3.\n",
        "- PyTorch Documentation: [Datasets and Data Loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n"
      ],
      "metadata": {
        "id": "sd6iFKF2gohh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Description\n",
        "---\n",
        "\n",
        "In this project, you are given an ML model that is pre-trained on a vulnerability dataset. The dataset consists of code samples labeled with True or False flags, depending on the presence and absense of a vulnerability. Your goal is to use the pre-trained model to predict if the code samples in the validation set contain vulnerabilities or not and analyse the results. Please proceed to the below tasks."
      ],
      "metadata": {
        "id": "ybWt0W4gjbiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Task 1*\n",
        "\n",
        "Build a data loader for the validation dataset present in the following path: \"*data_students/student_dataset.hdf5*\". You will be using this dataset to validate the performance of the ML model. The dataset is in HDF5 binary data format. This format is used to store large amount of data. Make sure that you import and familiarise yourself with the right Python libraries to handle HDF5 files.\n"
      ],
      "metadata": {
        "id": "IrciLvqNj96k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eem6AZNyyXsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5b9a1b-2783-4e80-c8fc-b05395721290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "Emcvn5XcIk3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Task 2*\n",
        "\n",
        "Generate a table with 10 random samples from the dataset and show their corresponding labels."
      ],
      "metadata": {
        "id": "ARwcBrbFlMu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/TUHH/CDS/Part-1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hspQPDqNyja",
        "outputId": "cbed4779-0379-4624-9d01-be741848186e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/TUHH/CDS/Part-1': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: display 10 random samples from the loaded dataset\n",
        "\n",
        "def show_random_samples(hdf5_path):\n",
        "    num_samples = 3  # Display only 3 samples\n",
        "    max_length = 100  # Truncate vectors to 100 characters\n",
        "\n",
        "file_path ='/content/drive/MyDrive/TUHH/CDS/PART-1/student_dataset.hdf5'\n",
        "\n",
        "with h5py.File(file_path, 'r') as f:\n",
        "  # Available keys\n",
        "  print(\"Available keys in HDF5 file:\", list(f.keys()))\n",
        "\n",
        "  # Extract datasets\n",
        "  vectors = f['vectors'][:]\n",
        "  labels = f['labels'][:]\n",
        "\n",
        "# Random selection of indices\n",
        "indices = np.random.choice(len(vectors), size=10, replace=False)\n",
        "\n",
        "# Sample Data\n",
        "sample_vectors = vectors[indices]\n",
        "sample_labels = labels[indices]\n",
        "\n",
        "# Build a DataFrame for display with truncated vectors\n",
        "df_samples = pd.DataFrame({\n",
        "            'Sample Index': indices,\n",
        "            'Vector (Truncated)': [repr(v[:10]) + '...' for v in sample_vectors],  # Truncate vectors\n",
        "            'Label': sample_labels\n",
        "})\n",
        "\n",
        "# Display the samples\n",
        "print(df_samples)"
      ],
      "metadata": {
        "id": "AuYminA_mTnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca961e7d-2ffe-4e0a-f4b0-d4fa32486db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available keys in HDF5 file: ['labels', 'source', 'vectors']\n",
            "   Sample Index                                 Vector (Truncated)  Label\n",
            "0           856  array([[ 1.40853599e-01,  6.09922826e-01, -2.8...   True\n",
            "1           495  array([[-7.24035561e-01,  3.26080024e-01, -9.6...  False\n",
            "2           682  array([[-3.66267025e-01,  2.06676662e-01, -6.6...  False\n",
            "3           516  array([[-5.26928425e-01, -5.97019613e-01,  2.7...  False\n",
            "4           451  array([[ 1.16817743e-01, -1.52544391e+00, -3.1...  False\n",
            "5           334  array([[-8.82684052e-01,  1.60595679e+00, -2.1...  False\n",
            "6           132  array([[ 1.37810397e+00, -1.19254696e+00, -3.1...  False\n",
            "7           467  array([[ 1.77723718e+00,  8.70357990e-01, -1.1...  False\n",
            "8           632  array([[ 1.17301333e+00, -3.04405779e-01,  1.9...   True\n",
            "9            20  array([[-8.5193080e-01, -2.7158836e-01, -3.201...  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Task 3*\n",
        "\n",
        "Inspect the dataset and answer the following questions:\n",
        "1.  How many samples are in the dataset?\n",
        "2. How many positive examples (vulnerability-labeled instances) are in the dataset?\n",
        "3. What is the vulnerable/non-vulnerable ratio?"
      ],
      "metadata": {
        "id": "da5YCWVkmUL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: inspect and understand the loaded dataset\n",
        "with h5py.File(file_path, 'r') as f:\n",
        "    # Extract labels dataset\n",
        "    labels = f['labels'][:]\n",
        "    # Total number of samples\n",
        "    total_samples = len(labels)\n",
        "\n",
        "    # Count the number of positive (vulnerable) samples (assuming '1' indicates vulnerability)\n",
        "    positive_samples = np.sum(labels == 1)\n",
        "\n",
        "    # Count the number of negative (non-vulnerable) samples\n",
        "    negative_samples = np.sum(labels == 0)\n",
        "\n",
        "    # Calculate the vulnerable/non-vulnerable ratio\n",
        "    vulnerability_ratio = positive_samples / negative_samples\n",
        "\n",
        "# Display the results\n",
        "print(f\"Total number of samples: {total_samples}\")\n",
        "print(f\"Number of vulnerable examples: {positive_samples}\")\n",
        "print(f\"Number of non-vulnerable examples: {negative_samples}\")\n",
        "print(f\"Vulnerable/Non-vulnerable ratio: {vulnerability_ratio}\")"
      ],
      "metadata": {
        "id": "LDpozMCfnnJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985af410-d59f-4eeb-869e-b34ddcac5a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 1000\n",
            "Number of vulnerable examples: 283\n",
            "Number of non-vulnerable examples: 717\n",
            "Vulnerable/Non-vulnerable ratio: 0.3947001394700139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Task 4*\n",
        "\n",
        "Load and run the following pre-trained neural network model called VulnPredictionModel."
      ],
      "metadata": {
        "id": "UivWlO3dnngr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "3Jex8XdkFJhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python\n",
        "from torch import nn\n",
        "\n",
        "class VulnPredictModel(nn.Module):\n",
        "    # intialize the model architecture\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_stack = nn.Sequential(\n",
        "         nn.Linear(768, 64),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(64, 64),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(64, 1),\n",
        "         nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "      # forward propagation\n",
        "      def forward(self, x):\n",
        "        pred = self.linear_stack(x)\n",
        "        return pred\n",
        "      \n",
        "\n",
        "# TODO: intialize and load the model\n",
        "```"
      ],
      "metadata": {
        "id": "9RrGtLkpEzKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "class VulnPredictModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Linear(768, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, x):\n",
        "        pred = self.linear_stack(x)\n",
        "        return pred\n",
        "\n",
        "# Initialize the model\n",
        "model = VulnPredictModel()\n",
        "\n",
        "# Move the model to device\n",
        "model.to(device)\n",
        "\n",
        "# Load the pre-trained weights\n",
        "model_path = '/content/drive/MyDrive/TUHH/CDS/PART-1/model_2023-03-28_20-03.pth'\n",
        "\n",
        "# Load the model weights into the model\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnPemogpUzCj",
        "outputId": "f4c0ba11-d752-4eaa-82ba-3de0cebb54e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VulnPredictModel(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_stack): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Task 5*\n",
        "\n",
        "Make a prediction on the provided dataset and compute the following values:\n",
        "- True Positives\n",
        "- True Negatives\n",
        "- False Positives\n",
        "- False Negatives"
      ],
      "metadata": {
        "id": "-A9M9ID0n2Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to PyTorch tensors\n",
        "vectors_tensor = torch.tensor(vectors, dtype=torch.float32)\n",
        "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# TODO: makethe prediction for all the samples in the validation set.\n",
        "with torch.no_grad():\n",
        "    # Move data to the same device as model\n",
        "    vectors_tensor = vectors_tensor.to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    outputs = model(vectors_tensor)\n",
        "    predictions = (outputs > 0.3).float().squeeze()\n",
        "\n",
        "    # Move labels to device for comparison\n",
        "    labels_tensor = labels_tensor.to(device)\n",
        "\n",
        "# todo: compute true positives, true negatives, false postives and false negatives.\n",
        "    true_positives = ((predictions == 1) & (labels_tensor == 1)).sum().item()\n",
        "    true_negatives = ((predictions == 0) & (labels_tensor == 0)).sum().item()\n",
        "    false_positives = ((predictions == 1) & (labels_tensor == 0)).sum().item()\n",
        "    false_negatives = ((predictions == 0) & (labels_tensor == 1)).sum().item()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"True Positives (TP): {true_positives}\")\n",
        "print(f\"True Negatives (TN): {true_negatives}\")\n",
        "print(f\"False Positives (FP): {false_positives}\")\n",
        "print(f\"False Negatives (FN): {false_negatives}\")\n"
      ],
      "metadata": {
        "id": "R8KdeQ2Rn-2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b09567-06c1-4902-97bc-3c5fb97c36b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix:\n",
            "True Positives (TP): 94\n",
            "True Negatives (TN): 704\n",
            "False Positives (FP): 13\n",
            "False Negatives (FN): 189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Task 6*\n",
        "\n",
        "Compute the corresponding performance metrics **manually** (do not use PyTorch's predefined metrics):\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1"
      ],
      "metadata": {
        "id": "TaFHwiVwow7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate accuracy\n",
        "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
        "\n",
        "# TODO: calculate precision\n",
        "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "\n",
        "# TODO: calculate recall\n",
        "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "\n",
        "# TODO: calculate F1-score\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Print the metrics\n",
        "print(\"\\nPerformance Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.5f} ({(accuracy*100):.1f}%)\")\n",
        "print(f\"Precision: {precision:.5f}\")\n",
        "print(f\"Recall: {recall:.5f}\")\n",
        "print(f\"F1 Score: {f1:.5f}\")\n"
      ],
      "metadata": {
        "id": "KE2daH3LpGEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88d5271-3315-4cac-e818-85f636b2b8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Metrics:\n",
            "Accuracy: 0.79800 (79.8%)\n",
            "Precision: 0.87850\n",
            "Recall: 0.33216\n",
            "F1 Score: 0.48205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Task 7*\n",
        "\n",
        "Based on your performance metrics, answer the following questions:\n",
        "\n",
        "- Explain the impact of accuracy vs. F1 score.\n",
        "- In this particular problem, which metric one should focus more on?\n",
        "- Is there a better metric suitable for the use case of vulnerability prediction? Why?\n"
      ],
      "metadata": {
        "id": "kdIkKUPlpGjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Impact of Accuracy vs. F1 Score\n",
        "While accuracy indicates the overall correctness of the model’s predictions (79.8% in our case), it can be misleading for imbalanced datasets like ours, where only 28% of the samples are actually labeled as vulnerable. A model could achieve high accuracy simply by predicting most samples as non-vulnerable, without actually detecting true vulnerabilities.\n",
        "The F1 score, on the other hand, provides a more balanced view by combining both precision (how many predicted vulnerabilities were correct) and recall (how many actual vulnerabilities were detected). In this project, the F1 score is significantly lower (0.48), revealing that the model struggles to identify many actual vulnerabilities, despite its high accuracy.\n",
        "\n",
        "2. Most Important Metric for This Problem\n",
        "For vulnerability detection, recall and F1 score are more important than accuracy. The reason is simple: missing a true vulnerability (false negative) is much more dangerous than falsely flagging safe code. Therefore, the goal should be to maximize the detection of true vulnerabilities, even if it means accepting some false positives.\n",
        "\n",
        "3. A Better-Suited Metric for Vulnerability Prediction\n",
        "In this context, recall is especially critical — we want to minimize the number of vulnerabilities that go undetected. The F1 score is also useful because it balances the need for high recall with maintaining reasonable precision.\n",
        "Alternatively, for more advanced evaluations in imbalanced classification tasks, metrics like Precision-Recall AUC (PR-AUC) or ROC-AUC may also be considered. However, for this project, focusing on F1 score and recall gives a more realistic picture of the model’s effectiveness in detecting security flaws.\n"
      ],
      "metadata": {
        "id": "TJIaqM2JO-vD"
      }
    }
  ]
}